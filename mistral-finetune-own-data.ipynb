{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFCx6jZU3m11"
   },
   "source": [
    "<!-- Banner Image -->\n",
    "<img src=\"https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdevnotebooks.png\" width=\"100%\">\n",
    "\n",
    "<!-- Links -->\n",
    "<center>\n",
    "  <a href=\"https://console.brev.dev\" style=\"color: #06b6d4;\">Console</a> â€¢\n",
    "  <a href=\"https://brev.dev\" style=\"color: #06b6d4;\">Docs</a> â€¢\n",
    "  <a href=\"/\" style=\"color: #06b6d4;\">Templates</a> â€¢\n",
    "  <a href=\"https://discord.gg/NVDyv7TUgJ\" style=\"color: #06b6d4;\">Discord</a>\n",
    "</center>\n",
    "\n",
    "# Fine-tuning Mistral on your own data ðŸ¤™\n",
    "\n",
    "Welcome!\n",
    "\n",
    "In this notebook and tutorial, we will fine-tune the [Mistral 7B](https://github.com/mistralai/mistral-src) model - which outperforms Llama 2 13B on all tested benchmarks - ***on your own data!***\n",
    "\n",
    "## Watch the accompanying video walk-through [here](https://youtu.be/kmkcNVvEz-k?si=Ogt1wRFNqYI6zXfw&t=1)!\n",
    "\n",
    "I did this for **just one dollar ($1)** on an 1x A10G 24GB from Brev.dev (instructions below).\n",
    "\n",
    "This tutorial will use QLoRA, a fine-tuning method that combines quantization and LoRA. For more information about what those are and how they work, see [this post](https://brev.dev/blog/how-qlora-works).\n",
    "\n",
    "In this notebook, we will load the large model in 4bit using `bitsandbytes` and use LoRA to train using the PEFT library from Hugging Face ðŸ¤—.\n",
    "\n",
    "Note that if you ever have trouble importing something from Huggingface, you may need to run `huggingface-cli login` in a shell. To open a shell in Jupyter Lab, click on 'Launcher' (or the '+' if it's not there) next to the notebook tab at the top of the screen. Under \"Other\", click \"Terminal\" and then run the command.\n",
    "\n",
    "### Help us make this tutorial better! Please provide feedback on the [Discord channel](https://discord.gg/RN2a436M73) or on [X](https://x.com/harperscarroll)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9TytWkb3m15"
   },
   "source": [
    "#### Before we begin: A note on OOM errors\n",
    "\n",
    "If you get an error like this: `OutOfMemoryError: CUDA out of memory`, tweak your parameters to make the model less computationally intensive. I will help guide you through that in this guide, and if you have any additional questions you can reach out on the [Discord channel](https://discord.gg/RN2a436M73) or on [X](https://x.com/harperscarroll).\n",
    "\n",
    "To re-try after you tweak your parameters, open a Terminal ('Launcher' or '+' in the nav bar above -> Other -> Terminal) and run the command `nvidia-smi`. Then find the process ID `PID` under `Processes` and run the command `kill [PID]`. You will need to re-start your notebook from the beginning. (There may be a better way to do this... if so please do let me know!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC-9m2yv3m18"
   },
   "source": [
    "## Let's begin!\n",
    "### 0. Preparing data\n",
    "\n",
    "Before you check out a GPU, prepare your dataset for loading and training.\n",
    "\n",
    "To prepare your dataset for loading, all you need are two `.jsonl` files structured something like this:\n",
    "```\n",
    "{\"input\": \"What color is the sky?\", \"output\": \"The sky is blue.\"}\n",
    "{\"input\": \"Where is the best place to get cloud GPUs?\", \"output\": \"Brev.dev\"}\n",
    "```\n",
    "If you choose to model your data as input/output pairs, you'll want to use something like the second `formatting_func` below, which will will combine all your features into one input string.\n",
    "\n",
    "As you can see below, I have `notes.jsonl` for my `train_dataset` and `notes_validation.jsonl` for my `eval_dataset`.\n",
    "\n",
    "I used Exporter, a free local-only app, to export my Apple Notes to `.txt` files, and then I wrote a script to process each note into one `.jsonl` file. Note that for this script, ChatGPT can help out a LOT if you tell it how your data is currently formatted, how you'd like it to be formatted, and ask it to write a script in a certain language you know well (for any debugging) to do so. I also broke up my journal entries so the training sample vector length was smaller (see the discussion on `max_length` and the data visualization below). I broke it into pieces so that contexts were encapsulated entirely, since I did want the model to understand context about my life. My data were ultimately formatted as:\n",
    "\n",
    "```json\n",
    "{\"note\": \"journal-entry-for-model-to-predict\"}\n",
    "{\"note\": \"journal-entry-for-model-to-predict-1\"}\n",
    "{\"note\": \"journal-entry-for-model-to-predict-2\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2CkxsA43m15"
   },
   "source": [
    "### 1. Instantiate GPU & Load Dataset\n",
    "\n",
    "I used a GPU and dev environment from [brev.dev](https://brev.dev). The whole thing cost me $1 using a 1xA10G 24GB. Click the badge below to get your preconfigured instance:\n",
    "\n",
    "[![](https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg)](https://console.brev.dev/environment/new?instance=A10G:g5.xlarge&diskStorage=256&name=mistral-finetune-own-data&file=https://github.com/brevdev/notebooks/raw/main/mistral-finetune-own-data.ipynb&python=3.10&cuda=12.0.1)\n",
    "\n",
    "A single A10G (as linked) with 24GB GPU Memory was enough for me. You may need more GPUs and/or Memory if your sequence max_length is larger than 512.\n",
    "\n",
    "Once you've checked out your machine and landed in your instance page, select the specs you'd like (I used **Python 3.10 and CUDA 12.0.1**; these should be preconfigured for you if you use the badge above) and click the \"Build\" button to build your verb container. Give this a few minutes.\n",
    "\n",
    "A few minutes after your model has started Running, click the 'Notebook' button on the top right of your screen once it illuminates (you may need to refresh the screen). You will be taken to a Jupyter Lab environment, where you can upload this Notebook.\n",
    "\n",
    "\n",
    "Note: You can connect your cloud credits (AWS or GCP) by clicking \"Org: \" on the top right, and in the panel that slides over, click \"Connect AWS\" or \"Connect GCP\" under \"Connect your cloud\" and follow the instructions linked to attach your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FuXIFTFapAMI",
    "outputId": "c8ced1ad-c7b3-44ba-807b-26d7d13906bc"
   },
   "outputs": [],
   "source": [
    "# You only need to run this once per machine\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s6f4z8EYmcJ6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='notes.jsonl', split='train')\n",
    "eval_dataset = load_dataset('json', data_files='data_validation.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05H5MIfjyRgc",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Accelerator\n",
    "\n",
    "Set up the Accelerator. I'm not sure if we really need this for a QLoRA given its [description](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/fsdp) (I have to read more about it) but it seems it can't hurt, and it's helpful to have the code for future reference. You can always comment out the accelerator if you want to try without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "TEzYBadkyRgd"
   },
   "outputs": [],
   "source": [
    "# from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "# from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "# fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "#     state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "#     optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "# )\n",
    "\n",
    "# accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9KNTJZkyRgn"
   },
   "source": [
    "\n",
    "Let's use Weights & Biases to track our training metrics. You'll need to apply an API key when prompted. Feel free to skip this if you'd like, and just comment out the `wandb` parameters in the `Trainer` definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "DDqUNyIoyRgo"
   },
   "outputs": [],
   "source": [
    "# !pip install -q wandb -U\n",
    "\n",
    "# import wandb, os\n",
    "# wandb.login()\n",
    "\n",
    "# wandb_project = \"journal-finetune\"\n",
    "# if len(wandb_project) > 0:\n",
    "#     os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhw8JiOr3m18"
   },
   "source": [
    "### Formatting prompts\n",
    "Then create a `formatting_func` to structure training examples as prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "f-fJR0MlQiTD"
   },
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### The following is a sample event: {example['input']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sflV0DL2P64_"
   },
   "source": [
    "Here's another common one:\n",
    "\n",
    "```python\n",
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
    "    return text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### 2. Load Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    "Let's now load Mistral - mistralai/Mistral-7B-v0.1 - using 4-bit quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /usr/users/quota/students/2021/dle3/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "login('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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running:  accelerate-launch /usr/users/quota/students/2021/dle3/miniconda3/envs/yap/lib/python3.11/site-packages/accelerate/test_utils/scripts/test_script.py\n",
      "stderr: The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "stderr: \t`--num_processes` was set to a value of `1`\n",
      "stderr: \t`--num_machines` was set to a value of `1`\n",
      "stderr: \t`--mixed_precision` was set to a value of `'no'`\n",
      "stderr: \t`--dynamo_backend` was set to a value of `'no'`\n",
      "stderr: To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "stdout: **Initialization**\n",
      "stdout: Testing, testing. 1, 2, 3.\n",
      "stdout: Distributed environment: DistributedType.NO\n",
      "stdout: Num processes: 1\n",
      "stdout: Process index: 0\n",
      "stdout: Local process index: 0\n",
      "stdout: Device: cuda\n",
      "stdout: \n",
      "stdout: Mixed precision type: no\n",
      "stdout: \n",
      "stdout: \n",
      "stdout: **Test process execution**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a list**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a dict**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a tensor**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a datasets.Dataset**\n",
      "stdout: \n",
      "stdout: **Test random number generator synchronization**\n",
      "stdout: All rng are properly synched.\n",
      "stdout: \n",
      "stdout: **DataLoader integration test**\n",
      "stdout: 0 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
      "stdout:        device='cuda:0') <class 'accelerate.data_loader.DataLoaderShard'>\n",
      "stdout: Non-shuffled dataloader passing.\n",
      "stdout: Shuffled dataloader passing.\n",
      "stdout: Non-shuffled central dataloader passing.\n",
      "stdout: Shuffled central dataloader passing.\n",
      "stdout: \n",
      "stdout: **Training integration test**\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
      "stdout: FP16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: BF16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
      "stdout: FP16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: BF16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: \n",
      "stdout: **Breakpoint trigger test**\n",
      "stdout: \n",
      "stdout: **Test reinstantiated state**\n",
      "Test is a success! You are ready for your distributed training!\n"
     ]
    }
   ],
   "source": [
    "! accelerate test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "45524c98039a46d5b7745ad7cb638d2f"
     ]
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "47b6b01d-e9f2-4b70-919c-17ae64993843"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69c982014384c3c83bdce1287d3e868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjNdXolqyRgf"
   },
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n",
    "\n",
    "\n",
    "For `model_max_length`, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "haSUDD9HyRgf",
    "outputId": "22ee95db-2974-4ab0-e0c7-444d04d3e838"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S3iLAwLh3m19"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1200a2da436b4d1eac67700f78d6a6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/93 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6ewk27p3m19"
   },
   "source": [
    "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BA8M9yfC3m19",
    "outputId": "99c6d302-9bb6-47b1-cae9-a1cd870b4770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBk4Qp_vyRgh"
   },
   "source": [
    "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs.\n",
    "\n",
    "I'm using my personal notes to train the model, and they vary greatly in length. I spent some time cleaning the dataset so the samples were about the same length, cutting up individual notes if needed, but being sure to not cut in the middle of a word or sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMlw8h743m19"
   },
   "source": [
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "acINaViR3m19"
   },
   "outputs": [],
   "source": [
    "max_length = 200 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "518d4f0b89bf4d57bf00d4c6d6e59eb5"
     ]
    },
    "id": "lTk-aTog3m19",
    "outputId": "4fb637b4-77a2-47c6-de7b-4fb620663dd7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442aa9c8433e4c83b1dca3c5d0bd6aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/93 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Check that `input_ids` is padded on the left with the `eos_token` (2) and there is an `eos_token` 2 added to the end, and the prompt starts with a `bos_token` (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OKHhvxK83m19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 774, 415, 2296, 349, 264, 7324, 1951, 28747, 3727, 13429, 382, 468, 19653, 28804, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6LRa2Zm3m19"
   },
   "source": [
    "Now all the samples should be the same length, `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "I55Yo3yy3m19",
    "outputId": "c87e344d-e0f3-4542-afcc-4e2025926d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP3R4enP3m19"
   },
   "source": [
    "### How does the base model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxbl4ACsyRgi"
   },
   "source": [
    "Optionally, you can check how Mistral does on one of your data samples. For example, if you have a dataset of users' biometric data to their health scores, you could test the following `eval_prompt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "gOxnx-cAyRgi"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\" Given the following biometric data, score the users' health, from 0-100.\n",
    "\n",
    "### Biometric Data:\n",
    "Temperature=98.2,\n",
    "Sex=F,\n",
    "Age=29,\n",
    "Height=69 inches,\n",
    "Weight=160 lbs,\n",
    "V02_Max=55,\n",
    "HRV=55\n",
    "\n",
    "### Health Score:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRhfq_Fa3m19"
   },
   "source": [
    "The `eval_prompt` I used was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pa6ux9ni3m19"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \" Networking event with CS Alumni # \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NidIuFXMyRgi",
    "outputId": "b1794b11-9a22-4b0a-e871-7df039ab59fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "2024-05-02 16:33:25.210194: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-02 16:33:32.789159: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-02 16:33:42.416801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Networking event with CS Alumni # 1\n",
      "\n",
      "On the 28th of November, we had our first networking event with alumni. The event was held at the University of Twente and it was a great success! We were able to connect with many alumni who are now working in various fields such as software engineering, data science, machine learning, and more. It was an excellent opportunity for students to learn about different career paths and get advice on how to navigate their own careers.\n",
      "\n",
      "We would like to thank all the alumni who attended this event and shared their experiences with us. We hope that this will be the start of many more events where we can continue to build relationships between current students and alumni.\n"
     ]
    }
   ],
   "source": [
    "# Init an eval tokenizer that doesn't add padding or eos token\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCAWeCzZyRgi"
   },
   "source": [
    "Observe how the model does out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 4. Set Up LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XshGNsbxyRgj",
    "outputId": "c619b0e8-8516-4d4b-9abe-13eaa3f3b204",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Ybeyl20n3dYH",
    "outputId": "6a16c182-04d9-4812-ae81-502a8fe364d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IaYMWak4yRgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 5. Run Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEe0uWYSyRgo"
   },
   "source": [
    "I didn't have a lot of training samples: only about 200 total train/validation. I used 500 training steps, and I was fine with overfitting in this case. I found that the end product worked well. It took about 20 minutes on the 1x A10G 24GB.\n",
    "\n",
    "Overfitting is when the validation loss goes up (bad) while the training loss goes down significantly, meaning the model is learning the training set really well, but is unable to generalize to new datapoints. In most cases, this is not desired, but since I am just playing around with a model to generate outputs like my journal entries, I was fine with a moderate amount of overfitting.\n",
    "\n",
    "With that said, a note on training: you can set the `max_steps` to be high initially, and examine at what step your model's performance starts to degrade. There is where you'll find a sweet spot for how many steps to perform. For example, say you start with 1000 steps, and find that at around 500 steps the model starts overfitting, as described above. Therefore, 500 steps would be your sweet spot, so you would use the `checkpoint-500` model repo in your output dir (`mistral-journal-finetune`) as your final model in step 6 below.\n",
    "\n",
    "If you're just doing something for fun like I did and are OK with overfitting, you can try different checkpoint versions with different degrees of overfitting.\n",
    "\n",
    "You can interrupt the process via Kernel -> Interrupt Kernel in the top nav bar once you realize you didn't need to train anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "c_L1131GyRgo"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "yxSbpKQSLY6B"
   },
   "outputs": [],
   "source": [
    "# model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2021/dle3/miniconda3/envs/yap/lib/python3.11/site-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "device() received an invalid combination of arguments - got (NoneType), but expected one of:\n * (torch.device device)\n      didn't match because some of the arguments have invalid types: (!NoneType!)\n * (str type, int index)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 36\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/transformers/trainer.py:1875\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1876\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/transformers/trainer.py:2032\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2030\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m   2031\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2032\u001b[0m         model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2034\u001b[0m     \u001b[38;5;66;03m# to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\u001b[39;00m\n\u001b[1;32m   2035\u001b[0m     model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[1;32m   2036\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\n\u001b[1;32m   2037\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/accelerate/accelerator.py:1289\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[38;5;66;03m# MS-AMP will handle the device placement\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m         device_placement \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m-> 1289\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m   1290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, first_pass\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[1;32m   1291\u001b[0m     )\n\u001b[1;32m   1292\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixed_precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp8\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_recipe_handler\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTE\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/accelerate/accelerator.py:1290\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[38;5;66;03m# MS-AMP will handle the device placement\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m         device_placement \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m   1289\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m-> 1290\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[1;32m   1291\u001b[0m     )\n\u001b[1;32m   1292\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixed_precision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp8\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_recipe_handler\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTE\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/accelerate/accelerator.py:1166\u001b[0m, in \u001b[0;36mAccelerator._prepare_one\u001b[0;34m(self, obj, first_pass, device_placement)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data_loader(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m-> 1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer):\n\u001b[1;32m   1168\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_optimizer(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/accelerate/accelerator.py:1396\u001b[0m, in \u001b[0;36mAccelerator.prepare_model\u001b[0;34m(self, model, device_placement, evaluation_mode)\u001b[0m\n\u001b[1;32m   1391\u001b[0m current_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(model_devices)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1392\u001b[0m current_device_index \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1393\u001b[0m     current_device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(current_device, torch\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01melse\u001b[39;00m current_device\n\u001b[1;32m   1394\u001b[0m )\n\u001b[0;32m-> 1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_device_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m   1397\u001b[0m     \u001b[38;5;66;03m# if on the first device (GPU 0) we don't care\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (current_device_index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m   1399\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1400\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded in 8-bit precision on a different device than the one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1401\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre training on. Make sure you loaded the model on the correct device using for example `device_map=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:torch.cuda.current_device() or device_map=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:torch.xpu.current_device()}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1402\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: device() received an invalid combination of arguments - got (NoneType), but expected one of:\n * (torch.device device)\n      didn't match because some of the arguments have invalid types: (!NoneType!)\n * (str type, int index)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"journal-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=500,\n",
    "        learning_rate=2.5e-5,  # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",          # Directory for storing logs\n",
    "        save_strategy=\"steps\",         # Save the model checkpoint every logging step\n",
    "        save_steps=25,                 # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\",   # Evaluate the model every logging step\n",
    "        eval_steps=25,                 # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                  # Perform evaluation at the end of training\n",
    "        report_to=\"none\"               # Ensure no external reporting\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9rRmDCeQiTJ"
   },
   "source": [
    "I cleared the output of the cell above because I stopped the training early, and it produced a long, ugly error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "### 6. Drum Roll... Try the Trained Model!\n",
    "\n",
    "It's a good idea to kill the current process so that you don't run out of memory loading the base model again on top of the model we just trained. Go to `Kernel > Restart Kernel` or kill the process via the Terminal (`nvidia smi` > `kill [PID]`). \n",
    "\n",
    "By default, the PEFT library will only save the QLoRA adapters, so we need to first load the base model from the Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "fb8230fb86884aa6be318e2d03a88af2"
     ]
    },
    "id": "SKSnF016yRgp",
    "outputId": "bce5209d-90da-4117-c6ac-cda9f3cb3422"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2019/anguyen/miniconda3/envs/yap/lib/python3.11/site-packages/accelerate/utils/modeling.py:1365: UserWarning: Current model requires 134218752.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m base_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      6\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Mistral, same as before\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Same quantization config as before\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m eval_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model_id, add_bos_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/transformers/modeling_utils.py:3632\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3629\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   3631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3632\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3635\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:86\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     83\u001b[0m         key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_not_convert\n\u001b[1;32m     84\u001b[0m     }\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "# DELETE THIS CELL. FIXED BELOW\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float1616\n",
    "# )\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model_id,  # Mistral, same as before\n",
    "#     quantization_config=bnb_config,  # Same quantization config as before\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "\n",
    "# eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c33123466854f6896306fe5663fc1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    load_in_8bit_fp32_cpu_offload=True  # Enable CPU offloading for parts of the model\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    # device_map=\"auto\",  # You can replace \"auto\" with a custom map if needed\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BxOhAiqyRgp"
   },
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "GwsiqhWuyRgp"
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacity of 11.73 GiB of which 11.12 MiB is free. Including non-PyTorch memory, this process has 11.69 GiB memory in use. Of the allocated memory 11.47 GiB is allocated by PyTorch, and 13.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[0;32m----> 3\u001b[0m ft_model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistral-journal-finetune/checkpoint-300\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/peft/peft_model.py:392\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[38;5;241m.\u001b[39mtask_type](model, config, adapter_name)\n\u001b[0;32m--> 392\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/peft/peft_model.py:858\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, torch_device, **kwargs)\u001b[0m\n\u001b[1;32m    855\u001b[0m         peft_config\u001b[38;5;241m.\u001b[39minference_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_adapter(adapter_name, peft_config)\n\u001b[0;32m--> 858\u001b[0m adapters_weights \u001b[38;5;241m=\u001b[39m \u001b[43mload_peft_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;66;03m# load the weights into the model\u001b[39;00m\n\u001b[1;32m    861\u001b[0m ignore_mismatched_sizes \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_mismatched_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:439\u001b[0m, in \u001b[0;36mload_peft_weights\u001b[0;34m(model_id, device, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m         adapters_weights \u001b[38;5;241m=\u001b[39m safe_load_file(filename, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         adapters_weights \u001b[38;5;241m=\u001b[39m \u001b[43msafe_load_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     adapters_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(filename, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(device))\n",
      "File \u001b[0;32m~/miniconda3/envs/yap/lib/python3.11/site-packages/safetensors/torch.py:313\u001b[0m, in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m safe_open(filename, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 313\u001b[0m         result[k] \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mget_tensor(k)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacity of 11.73 GiB of which 11.12 MiB is free. Including non-PyTorch memory, this process has 11.69 GiB memory in use. Of the allocated memory 11.47 GiB is allocated by PyTorch, and 13.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-journal-finetune/checkpoint-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX39ibolyRgp"
   },
   "source": [
    "and run your inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUehsaVNyRgp"
   },
   "source": [
    "Let's try the same `eval_prompt` and thus `model_input` as above, and see if the new finetuned model performs better. I like playing with the repetition penalty (just little tweaks of .01-.05 at a time). THIS IS SO FUN. I'm obsessed wth this AI version of myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "lMkVNEUvyRgp",
    "outputId": "7d49d409-5dbe-4306-c1a4-9d87e3073397"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ft_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m eval_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Networking event with CS Alumni # \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m model_input \u001b[38;5;241m=\u001b[39m eval_tokenizer(eval_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mft_model\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(eval_tokenizer\u001b[38;5;241m.\u001b[39mdecode(ft_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_input, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.15\u001b[39m)[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ft_model' is not defined"
     ]
    }
   ],
   "source": [
    "eval_prompt = \" Networking event with CS Alumni # \"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCJnpZoayRgq"
   },
   "source": [
    "### Sweet... it worked! The fine-tuned model now prints out journal entries in my style!\n",
    "\n",
    "How funny to see it write like me as an angsty teenager, and honestly adult. I am obsessed. It knows who my friends are and talks about them, and covers the same topics I usually cover. It's really cool.\n",
    "\n",
    "That output is quite private but I wanted you to see an example run, so I tweaked the `eval_prompt` so that it explicitly wouldn't say anything too sensitive, haha.\n",
    "\n",
    "I hope you enjoyed this tutorial on fine-tuning Mistral on your own data. If you have any questions, feel free to reach out to us on [X](https://x.com/brevdev) or [Discord](https://discord.gg/RN2a436M73).\n",
    "\n",
    "ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
